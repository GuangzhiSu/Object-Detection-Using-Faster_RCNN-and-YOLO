{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from detectron2.engine import DefaultTrainer, HookBase\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the original COCO annotations JSON file\n",
    "with open(\"/home/gs285/AIPI_HW/Faster RCNN/annotations_COCO/laptop_annotation.json\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Get all image IDs from the dataset\n",
    "image_ids = [img['id'] for img in coco_data['images']]\n",
    "\n",
    "# Split into 80% training and 20% validation\n",
    "train_ids, val_ids = train_test_split(image_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "def filter_annotations_by_image_ids(coco_data, image_ids):\n",
    "    # Filter images\n",
    "    filtered_images = [img for img in coco_data['images'] if img['id'] in image_ids]\n",
    "    \n",
    "    # Filter annotations\n",
    "    filtered_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in image_ids]\n",
    "    \n",
    "    # Preserve the other fields (categories, etc.)\n",
    "    filtered_data = {\n",
    "        \"images\": filtered_images,\n",
    "        \"annotations\": filtered_annotations,\n",
    "        \"categories\": coco_data['categories']\n",
    "    }\n",
    "    return filtered_data\n",
    "\n",
    "# Create new JSON files for train and val sets\n",
    "train_data = filter_annotations_by_image_ids(coco_data, train_ids)\n",
    "val_data = filter_annotations_by_image_ids(coco_data, val_ids)\n",
    "\n",
    "# Save the new annotation files\n",
    "os.makedirs(\"split_annotation\", exist_ok=True)\n",
    "with open(\"split_annotation/annotations_train.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(\"split_annotation/annotations_val.json\", \"w\") as f:\n",
    "    json.dump(val_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization during the training phase on validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook to evaluate and visualize the validation set every 100 iterations\n",
    "class ValidationVisualizationHook(HookBase):\n",
    "    def __init__(self, cfg, predictor, eval_period=100):\n",
    "        self.cfg = cfg\n",
    "        self.predictor = predictor\n",
    "        self.eval_period = eval_period\n",
    "\n",
    "    def after_step(self):\n",
    "        iteration = self.trainer.iter\n",
    "        if iteration % self.eval_period == 0 and iteration != 0:\n",
    "            self.visualize_predictions()\n",
    "\n",
    "    def visualize_predictions(self):\n",
    "        # Get validation dataset\n",
    "        dataset_dicts = DatasetCatalog.get(self.cfg.DATASETS.TEST[0])\n",
    "        metadata = MetadataCatalog.get(self.cfg.DATASETS.TEST[0])\n",
    "\n",
    "        # Visualize predictions on the first 5 validation images\n",
    "        for d in dataset_dicts[:5]:\n",
    "            img = cv2.imread(d[\"file_name\"])\n",
    "            outputs = self.predictor(img)\n",
    "\n",
    "            # Visualize the predicted bounding boxes\n",
    "            visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.8, instance_mode=ColorMode.IMAGE_BW)\n",
    "            out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "            # Display the results\n",
    "            cv2.imshow(\"Validation Predictions\", out.get_image()[:, :, ::-1])\n",
    "            cv2.waitKey(1)  # Wait a moment before moving to the next image\n",
    "\n",
    "            # Optionally, save the visualized image\n",
    "            output_path = os.path.join(self.cfg.OUTPUT_DIR, f\"validation_vis_iter_{self.trainer.iter}.jpg\")\n",
    "            cv2.imwrite(output_path, out.get_image()[:, :, ::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detector Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Detector:\n",
    "    def __init__(self):\n",
    "        # Register datasets\n",
    "        register_coco_instances(\"my_dataset_train\", {}, \"/home/gs285/AIPI_HW/Faster RCNN/output/annotations_train.json\", \"/home/gs285/AIPI_HW/image_dataset\")\n",
    "        register_coco_instances(\"my_dataset_val\", {}, \"/home/gs285/AIPI_HW/Faster RCNN/output/annotations_val.json\", \"/home/gs285/AIPI_HW/image_dataset\")\n",
    "\n",
    "        # Create configuration\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "        self.cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
    "        self.cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "        self.cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
    "        self.cfg.MODEL.DEVICE = 'cuda'\n",
    "        self.cfg.SOLVER.MAX_ITER = 1000\n",
    "        self.cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "        self.cfg.SOLVER.BASE_LR = 0.00025\n",
    "        self.cfg.SOLVER.WARMUP_ITERS = 0\n",
    "        self.cfg.SOLVER.CLIP_GRADIENTS.ENABLED = True\n",
    "        self.cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
    "        self.cfg.OUTPUT_DIR = \"./output\"\n",
    "\n",
    "        # Create trainer and predictor\n",
    "        self.trainer = CustomTrainer(self.cfg)\n",
    "        self.predictor = DefaultPredictor(self.cfg)\n",
    "\n",
    "        # Add custom hooks (visualize predictions every 100 iterations)\n",
    "        self.trainer.register_hooks([ValidationVisualizationHook(self.cfg, self.predictor, eval_period=100)])\n",
    "\n",
    "    def train(self):\n",
    "        # Resume or start training\n",
    "        self.trainer.resume_or_load(resume=False)\n",
    "        self.trainer.train()\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Run evaluation on validation set\n",
    "        self.trainer.test(self.cfg, self.trainer.model)\n",
    "\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "# Instantiate and run the detector\n",
    "detector = Detector()\n",
    "detector.train()\n",
    "detector.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
