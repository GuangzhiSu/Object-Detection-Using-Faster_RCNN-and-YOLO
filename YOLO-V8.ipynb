{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths to the original folder and the new folder\n",
    "original_folder = \"/home/gs285/AIPI_HW/annotations_utensil\"\n",
    "new_folder = \"/home/gs285/AIPI_HW/modified_annotations_utensil\"\n",
    "\n",
    "# Create the new folder if it doesn't exist\n",
    "os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all .txt files in the original folder\n",
    "for filename in os.listdir(original_folder):\n",
    "    if filename.endswith(\".txt\"):  # Process only .txt files\n",
    "        original_file_path = os.path.join(original_folder, filename)\n",
    "        new_file_path = os.path.join(new_folder, filename)\n",
    "\n",
    "        # Read the content of the original file\n",
    "        with open(original_file_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Modify the class ID (first element) in each line to 3\n",
    "        updated_lines = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if parts:  # Ensure the line is not empty\n",
    "                parts[0] = \"3\"  # Change the class ID to 3\n",
    "                updated_lines.append(\" \".join(parts) + \"\\n\")\n",
    "\n",
    "        # Write the modified content to the new file\n",
    "        with open(new_file_path, \"w\") as new_file:\n",
    "            new_file.writelines(updated_lines)\n",
    "\n",
    "print(f\"All annotation files have been modified and saved to: {new_folder}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 240 images\n",
      "Validation set: 60 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "images_folder = \"yolo_data/images\"\n",
    "labels_folder = \"yolo_data/labels\"\n",
    "train_images_folder = \"yolo_data_split/images/train\"\n",
    "val_images_folder = \"yolo_data_split/images/val\"\n",
    "train_labels_folder = \"yolo_data_split/labels/train\"\n",
    "val_labels_folder = \"yolo_data_split/labels/val\"\n",
    "\n",
    "# Create directories for train and val splits\n",
    "os.makedirs(train_images_folder, exist_ok=True)\n",
    "os.makedirs(val_images_folder, exist_ok=True)\n",
    "os.makedirs(train_labels_folder, exist_ok=True)\n",
    "os.makedirs(val_labels_folder, exist_ok=True)\n",
    "\n",
    "# Get list of image files and corresponding labels\n",
    "image_files = os.listdir(images_folder)\n",
    "image_files = [f for f in image_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'))]  # Filter out only the image files\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8  # 80% for training, 20% for validation\n",
    "split_index = int(len(image_files) * train_ratio)\n",
    "\n",
    "# Split the data\n",
    "train_images = image_files[:split_index]\n",
    "val_images = image_files[split_index:]\n",
    "\n",
    "# Move images and labels to corresponding folders\n",
    "def move_files(image_list, destination_image_folder, destination_label_folder):\n",
    "    for image_file in image_list:\n",
    "        # Move the image file\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        shutil.copy(image_path, os.path.join(destination_image_folder, image_file))\n",
    "        \n",
    "        # Move the corresponding label file\n",
    "        label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "        if os.path.exists(label_path):\n",
    "            shutil.copy(label_path, os.path.join(destination_label_folder, label_file))\n",
    "        else:\n",
    "            print(f\"Warning: Label for {image_file} not found. Skipping.\")\n",
    "\n",
    "# Move training data\n",
    "move_files(train_images, train_images_folder, train_labels_folder)\n",
    "\n",
    "# Move validation data\n",
    "move_files(val_images, val_images_folder, val_labels_folder)\n",
    "\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize predictions on the validation set\n",
    "def visualize_predictions(model, val_images_folder, epoch):\n",
    "    print(f\"\\nVisualizing predictions at epoch {epoch}...\")\n",
    "    # Select a few validation images to visualize predictions\n",
    "    for i, image_file in enumerate(os.listdir(val_images_folder)[:8]):  # Use first 8 images\n",
    "        if not image_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')):\n",
    "            print(f\"Skipping unsupported file: {image_file}\")\n",
    "            continue  # Skip non-image files\n",
    "        \n",
    "        image_path = os.path.join(val_images_folder, image_file)\n",
    "        results = model(image_path)  # Generate predictions\n",
    "        result_img = results[0].plot()  # This renders an annotated NumPy image\n",
    "        \n",
    "        # Display predictions using matplotlib\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(result_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Predictions at Epoch {epoch}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training from epoch 1 to 1...\n",
      "New https://pypi.org/project/ultralytics/8.3.18 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.1 ðŸš€ Python-3.8.20 torch-2.4.1+cu121 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/gs285/AIPI_HW/custom_dataset.yaml, epochs=1, time=None, patience=100, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=yolov8_custom30, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolov8_custom30\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    431452  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 249 layers, 2,690,988 parameters, 2,690,972 gradients, 6.9 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/yolov8_custom30', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/gs285/AIPI_HW/yolo_data_split/labels/train.cache... 240 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/gs285/AIPI_HW/yolo_data_split/images/train/notebook-xps-13-9340-t-sl-gallery-2 (1).png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     3.4629      5.4863       6.403      8.2337      3.4307      7.8165      5.1152      1.5922]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/gs285/AIPI_HW/yolo_data_split/labels/val.cache... 60 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/yolov8_custom30/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/yolov8_custom30\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      1.33G      1.269       3.56       1.47         47        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  7.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         60        144     0.0383      0.521      0.112     0.0494\n",
      "\n",
      "1 epochs completed in 0.003 hours.\n",
      "Optimizer stripped from runs/detect/yolov8_custom30/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from runs/detect/yolov8_custom30/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating runs/detect/yolov8_custom30/weights/best.pt...\n",
      "Ultralytics 8.3.1 ðŸš€ Python-3.8.20 torch-2.4.1+cu121 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "Model summary (fused): 186 layers, 2,685,148 parameters, 0 gradients, 6.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         60        144     0.0401      0.521       0.11     0.0522\n",
      "                laptop         51        118     0.0769      0.119      0.111     0.0519\n",
      "                 mouse         23         26    0.00326      0.923      0.108     0.0525\n",
      "Speed: 0.2ms preprocess, 4.9ms inference, 0.0ms loss, 3.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolov8_custom30\u001b[0m\n",
      "Ultralytics 8.3.1 ðŸš€ Python-3.8.20 torch-2.4.1+cu121 CUDA:0 (NVIDIA TITAN Xp, 12190MiB)\n",
      "Model summary (fused): 186 layers, 2,685,148 parameters, 0 gradients, 6.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/gs285/AIPI_HW/yolo_data_split/labels/val.cache... 60 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         60        144     0.0318      0.491      0.103     0.0513\n",
      "                laptop         51        118     0.0604      0.136      0.111     0.0558\n",
      "                 mouse         23         26    0.00318      0.846     0.0959     0.0469\n",
      "Speed: 0.3ms preprocess, 3.0ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolov8_custom302\u001b[0m\n",
      "Validation mAP@0.5:0.95: 0.05133518285984482\n",
      "Validation mAP@0.5: 0.10347281984538868\n",
      "Training speed (ms/img): 3.033188978830973\n",
      "Model size: 5.26 MB\n",
      "Ultralytics 8.3.1 ðŸš€ Python-3.8.20 torch-2.4.1+cu121 CPU (Intel Xeon E5-2687W v4 3.00GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/detect/yolov8_custom30/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 8, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.34...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.1s, saved as 'runs/detect/yolov8_custom30/weights/best.onnx' (10.5 MB)\n",
      "\n",
      "Export complete (1.2s)\n",
      "Results saved to \u001b[1m/home/gs285/AIPI_HW/runs/detect/yolov8_custom30/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs/detect/yolov8_custom30/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=runs/detect/yolov8_custom30/weights/best.onnx imgsz=640 data=/home/gs285/AIPI_HW/custom_dataset.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/detect/yolov8_custom30/weights/best.onnx'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the pre-trained YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Path to validation images\n",
    "val_images_folder = \"yolo_data_split/images/val\"\n",
    "\n",
    "# Set total number of epochs and batch size\n",
    "total_epochs = 1\n",
    "epoch_step = 1  # Number of epochs per incremental training step\n",
    "\n",
    "for epoch in range(0, total_epochs, epoch_step):\n",
    "    print(f\"\\nTraining from epoch {epoch + 1} to {epoch + epoch_step}...\")\n",
    "    # Train the model for epoch_step epochs\n",
    "    results = model.train(\n",
    "        data=\"/home/gs285/AIPI_HW/custom_dataset.yaml\",  # Path to the custom dataset YAML file\n",
    "        epochs=epoch_step,  # Train for this many epochs in each step\n",
    "        batch=8,  # Batch size\n",
    "        imgsz=640,  # Image size for training\n",
    "        name=\"yolov8_custom\"  # Name of the experiment run\n",
    "    )\n",
    "    # # Visualize predictions on the validation set every 5 epochs\n",
    "    # visualize_predictions(model, val_images_folder, epoch + epoch_step)\n",
    "\n",
    "# Evaluate the final model on the validation set\n",
    "val_results = model.val()\n",
    "\n",
    "# # Extract performance metrics from the results_dict() method\n",
    "results_dict = val_results.results_dict\n",
    "\n",
    "# Get mAP, speed, and model size\n",
    "mAP_50_95 = results_dict.get(\"metrics/mAP50-95(B)\", None)  # mAP across IoUs\n",
    "mAP_50  = results_dict.get(\"metrics/mAP50(B)\", None)  # Use .get() to avoid KeyErrors\n",
    "\n",
    "# Extract speed (inference time per image in ms)\n",
    "speed = val_results.speed.get(\"inference\", None)  # Speed in ms/img\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Validation mAP@0.5:0.95: {mAP_50_95}\")\n",
    "print(f\"Validation mAP@0.5: {mAP_50}\")\n",
    "print(f\"Training speed (ms/img): {speed}\")\n",
    "\n",
    "\n",
    "# Save the model temporarily to get its size\n",
    "temp_model_path = \"temp_model.pt\"\n",
    "model.save(temp_model_path)  # Save the model weights\n",
    "# Get the size in MB\n",
    "model_size = os.path.getsize(temp_model_path) / (1024 * 1024)  # Convert bytes to MB\n",
    "# Clean up the temporary file\n",
    "os.remove(temp_model_path)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "\n",
    "# Export the trained model to ONNX format\n",
    "model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
